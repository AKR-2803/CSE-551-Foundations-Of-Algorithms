### P vs NP

- **P**: Problems that can be solved in polynomial time using a deterministic Turing machine.  
- **NP**: Problems for which a solution can be verified in polynomial time using a deterministic Turing machine or solved using a non-deterministic Turing machine.

#### Definitions:
- **NP-hard**: A problem is NP-hard if every problem in NP can be reduced to it in polynomial time. NP-hard problems are at least as hard as the hardest problems in NP but do not have to belong to NP themselves.  
- **NP-complete**: A problem is NP-complete if it is both in NP and NP-hard. These are the most challenging problems in NP, as solving one NP-complete problem efficiently means all NP problems can be solved efficiently.

___

### Approximation Algorithms

- **Error Bound**: \((|APP(I) - OPT(I)|) / OPT(I) \leq \epsilon\) for all instances, where \(APP(I)\) is the approximate solution, \(OPT(I)\) is the optimal solution, and \(\epsilon\) is the error.
- **Error**: The deviation between the approximate solution and the optimal solution. The aim is to minimize the error.
  - The algorithm might occasionally produce a solution with no error, but there is no guarantee.  
  - In the worst case, the error could be large (e.g., 5000%), which would make the algorithm ineffective.
- **Goal**: Guarantee that the error is not greater than a specific percentage (\(X\%\)) for all instances.
  - The challenge lies in claiming or proving this bound for an infinite number of instances without testing them all.
  - Providing a mathematical bound for the error can help establish this guarantee.

---

### Key Points:
1. **OPT** is the optimal solution with zero error but may take an impractical amount of time (e.g., centuries) to compute.
2. **APP** is the approximate solution with an error bound. It is computationally feasible to compute compared to \(OPT\).
3. The goal is to approximate \(\frac{APP(I_{TSP})}{OPT(I_{TSP})} \leq 1.2\) without explicitly knowing \(OPT(I_{TSP})\).

---

### Travelling Salesman Problem (TSP):
1. For any tour in TSP, there must be \(n\) edges.  
2. **Lower Bound**: \(OPT(I_{TSP}) \geq X\), where \(X = e_1 + e_2 + \dots + e_n\) (edge weights of \(n\) minimum spanning edges).
3. **Upper Bound**: \(APP(I_{TSP}) \leq Y\).  
4. Error Ratio: \(\frac{APP(I_{TSP})}{OPT(I_{TSP})} \leq \frac{Y}{X}\).  

This approach ensures that even without explicitly knowing \(OPT(I_{TSP})\), a bound can be established to measure the approximation's quality.